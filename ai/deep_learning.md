# Deep Learning

## Notes

### 2023/12/26
["Attention", "Transformers", in Neural Network "Large Language Models"](http://bactra.org/notebooks/nn-attention-and-transformers.html) 
is a really helpful writeup explaining the concept of attention and the transformer and
LLM architecture. Especially for an audience with a mathematical bend.


## Resources
- [Attention is all you need](https://arxiv.org/abs/1706.03762)
- ["Attention", "Transformers", in Neural Network "Large Language Models"](http://bactra.org/notebooks/nn-attention-and-transformers.html)
